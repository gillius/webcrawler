# Web Crawler Project

This is a simple project that will scan all pages and content of a domain starting from an HTML page served
at /. It will ignore content on other domains.

## License

This project is licensed under the terms of the "MIT License".

## TODOs

* Hash content so that the same content at different pages can be detected as the same thing and we
  don't re-index it.
* Use a HEAD before GET in case the content is big or not HTML.
* Support a max-depth option.
* Simple static server for testing the actual sites with http URLs.
* Find out the tags that a bot like Google puts on requests that would encourage sites to return a
  server-side generated page for pages generated by JavaScript.
* Executor to load pages in parallel.
  
## Not In Scope
* Some way to support links that might be in scripts or generated by scripts? That would require running
  the full front-end code of the site, which we don't want to do.
* Parsing SVG to find links in them.
* Full evaluation to ensure no security vulnerabilities with JSoup exist when parsing malicious inputs. Given the
  sandbox nature of the JVM and limits on max processing size in JSoup, likely the worst scenario is some kind of
  CPU-based DoS, which has limited impact as this is intended to be run as an interactive CLI application.