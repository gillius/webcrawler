# Web Crawler Project

This is a simple project that will scan all pages and content of a domain starting from an HTML page served
at /. It will ignore content on other domains.

## License

This project is licensed under the terms of the "MIT License".

## TODOs

* Hash content so that the same content at different pages can be detected as the same thing and we
  don't re-index it.
* Use a HEAD before GET in case the content is big or not HTML.
* Support a max-depth option.
* Simple static server for testing the actual sites with http URLs.
* Find out the tags that a bot like Google puts on requests that would encourage sites to return a
  server-side generated page for pages generated by JavaScript.
  
## Not In Scope
* Some way to support links that might be in scripts or generated by scripts? That would require running
  the full front-end code of the site, which we don't want to do.
* Parsing SVG to find links in them.