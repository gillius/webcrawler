# Web Crawler Project

This is a simple project that will scan all pages and content reachable by an HTTP/HTTPS or file URL. It will ignore
content on other domains.

![test](https://github.com/gillius/webcrawler/workflows/test/badge.svg?branch=main)

## License

This project is licensed under the terms of the "MIT License".

## Usage

In order to run tbe webcrawler, you need the following:

* JDK 11 or later on your path, or with JAVA_HOME pointing to the installation. You can download from
  [AdoptOpenJDK](https://adoptopenjdk.net/?variant=openjdk11&jvmVariant=hotspot).
* Internet connection to be able to download artifacts to build at https://repo1.maven.org/maven2/.

Usage output:

    Usage: ./webcrawler <options>
      -f, --file=PARAM         Load a site from a local file path
      -h, --help               Display usage
          -json                Output to JSON format instead of text format
      -o, --outputFile=PARAM   Write output to specified file
          -pretty              When combined with -json, pretty-prints the output.
                                 Note JSON output is buffered in memory so do not
                                 use with huge outputs.
      -t, --threads=PARAM      The number of threads to use for processing (default
                                 1)
      -u, --url=PARAM          Load a site from a URL
    At least one of -f or -u options required.
    Output goes to stdout, unless -o specified; logs go to stderr

Example crawl included test site: `./webcrawler.sh ./webcrawler -f src/test/resources/simple-site/index.html`

Windows systems: just use `webcrawler` as the command.

*nix systems: If you are running on a *nix system where Bash is not installed, you can run the command through gradle
directly: `./gradlew run --args="-f whatever"`

Running from IDE: import the Gradle project in your favorite IDE such as [IntelliJ IDEA](https://www.jetbrains.com/idea/)
and run the WebCrawler class's main method.

## TODOs

* Hash content so that the same content at different pages can be detected as the same thing and we
  don't re-index it.
* Use a HEAD before GET in case the content is big or not HTML.
* Support a max-depth option.
* Simple static server for testing the actual sites with http URLs.
* Find out the tags that a bot like Google puts on requests that would encourage sites to return a
  server-side generated page for pages generated by JavaScript.
* Make sure that we have a way to limit the amount of data we process in the non-local case.
* Allow a limit on the maximum time to resolve a FutureResource
  
## Not In Scope -- Future Work
* Some way to support links that might be in scripts or generated by scripts? That would require running
  the full front-end code of the site, which we don't want to do.
* Parsing SVG to find links in them.
* Full evaluation to ensure no security vulnerabilities with JSoup exist when parsing malicious inputs. Given the
  sandbox nature of the JVM and limits on max processing size in JSoup, likely the worst scenario is some kind of
  CPU-based DoS, which has limited impact as this is intended to be run as an interactive CLI application.
* Loose comparison of domains when detecting external links, such as links to example.com from www.example.com
  (alias domains), or links to images.example.com from example.com (CDN domains)
  * This includes also detection of links to http://example.com:80/ from http://example.com as "internal" and likewise
    for https and port 443.
